{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Đọc ảnh gốc\n",
    "image = cv2.imread(r'D:\\PROJECTs\\Handwritten Digit\\test_image_1.jpg')\n",
    "\n",
    "\n",
    "# Chuyển đổi sang ảnh xám\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Thay đổi kích thước thành 28x28 pixel\n",
    "resized = cv2.resize(gray, (28, 28), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "inverted = 255 - resized\n",
    "\n",
    "# Lưu ảnh kết quả\n",
    "cv2.imwrite('anh_xam_28x28.jpg', resized)\n",
    "\n",
    "# Hiển thị ảnh (tùy chọn)\n",
    "cv2.imshow('Anh xam 28x28', resized)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Visualization tools\n",
    "import torchvision\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import torchvision.transforms.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\m'\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_13960\\2631827028.py:1: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  model = torch.load('.\\model.pth')\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_13960\\2631827028.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('.\\model.pth')\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('.\\model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PIL.Image.Image"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chuyển sang PIL Image\n",
    "img = F.to_pil_image(inverted)\n",
    "type(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APCKu2mlXd6R5KLg85ZwOPp1qkeDiiiug8J/8f0v+4P/AEIVz9FFb/hsraXDzTyRojKMEuM9c9M5rEmheFsPt5/usG/lUdFSROqBt0KSZ6bi3H5EVHRX/9k=",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA+0lEQVR4AXWSPQ7CMAyFnf6piA4sCAnEBZiYuf/AFTqDQGKlCBAqwXac2LSqhyb25+e8tHUzmI5sGgFY6P8aMbPQWegxUzjQUadCyortPg9ynlKkUdVmuWhedfsE8MAsKnHmqjy33Ru+qRsHpSkngHXZux4LQSdnGi9Zj0NjsKHYSUVPd5BQt6EQvXA2hCpDPIQyMCwjaKUjaKUW6h2oAzMDzR0I4nyFyMx5PCVB0hGUBl4EhveSefCpGxvtHnIx4cWaS3/frq6bzlfF43m9kSEM/mTs80Ifi0JkuCPl4A7cwQ86UwxqMe70H8LKYX6HYyS4qiFMsvxjEMAPXYszgBRjPZAAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted = F.to_tensor(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3686)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\m'\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_13960\\99738698.py:2: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  model = torch.load('.\\model.pth')\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_13960\\99738698.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('.\\model.pth')\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = torch.load('.\\model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def predict_image(image_path, model):\n",
    "    # Chuyển đổi và chuẩn bị ảnh\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(),\n",
    "        transforms.Resize((28, 28)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    image = Image.open(image_path)\n",
    "    image = transform(image).unsqueeze(0)\n",
    "\n",
    "    # show image\n",
    "    plt.imshow(image.squeeze().numpy(), cmap='gray')\n",
    "\n",
    "    # to device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    image = image.to(device)\n",
    "    \n",
    "    # Chuyển mô hình sang chế độ đánh giá\n",
    "    model.eval()\n",
    "    \n",
    "    # Dự đoán\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        \n",
    "    # Lấy kết quả dự đoán\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    return predicted.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [1, 28, 28] doesn't match the broadcast shape [3, 28, 28]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Dự đoán ảnh\u001b[39;00m\n\u001b[0;32m      2\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_image_3.png\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDự đoán: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[95], line 18\u001b[0m, in \u001b[0;36mpredict_image\u001b[1;34m(image_path, model)\u001b[0m\n\u001b[0;32m     10\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m     11\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mGrayscale(),\n\u001b[0;32m     12\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m)),\n\u001b[0;32m     13\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m     14\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])\n\u001b[0;32m     15\u001b[0m ])\n\u001b[0;32m     17\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(image_path)\n\u001b[1;32m---> 18\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# show image\u001b[39;00m\n\u001b[0;32m     21\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(image\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mnumpy(), cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\PROJECTs\\Handwritten Digit\\venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32md:\\PROJECTs\\Handwritten Digit\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\PROJECTs\\Handwritten Digit\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\PROJECTs\\Handwritten Digit\\venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\PROJECTs\\Handwritten Digit\\venv\\Lib\\site-packages\\torchvision\\transforms\\functional.py:350\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 350\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\PROJECTs\\Handwritten Digit\\venv\\Lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:926\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m std\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    925\u001b[0m     std \u001b[38;5;241m=\u001b[39m std\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 926\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdiv_(std)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: output with shape [1, 28, 28] doesn't match the broadcast shape [3, 28, 28]"
     ]
    }
   ],
   "source": [
    "# Dự đoán ảnh\n",
    "image_path = 'test_image_3.png'\n",
    "prediction = predict_image(image_path, model)\n",
    "print(f\"Dự đoán: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'collections.OrderedDict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minverted\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'collections.OrderedDict' object is not callable"
     ]
    }
   ],
   "source": [
    "model(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0314, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0314, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0314, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0314, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0314, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0510, 0.1490, 0.2235, 0.0471, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0588, 0.1647, 0.2471,\n",
       "         0.3098, 0.3569, 0.3255, 0.3569, 0.0706, 0.0353, 0.0353, 0.0353, 0.0314,\n",
       "         0.0314, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0314, 0.0353, 0.0353, 0.1490, 0.1843,\n",
       "         0.1373, 0.1569, 0.3098, 0.2980, 0.3569, 0.0627, 0.0353, 0.0353, 0.0353,\n",
       "         0.0314, 0.0314, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.1373, 0.3294, 0.2980, 0.3608, 0.0510, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0314, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.1373, 0.3294, 0.2980, 0.3686, 0.0392, 0.0353,\n",
       "         0.0314, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.1373, 0.3294, 0.2980, 0.3686, 0.0353,\n",
       "         0.0314, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.1373, 0.3294, 0.2980, 0.3647,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.1373, 0.3294, 0.2980,\n",
       "         0.3686, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.1373, 0.3294,\n",
       "         0.2980, 0.3686, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.1373,\n",
       "         0.3294, 0.2980, 0.3686, 0.0392, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.1373, 0.3294, 0.2980, 0.3686, 0.0392, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0314, 0.1373, 0.3294, 0.2980, 0.3686, 0.0392, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0314, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0314, 0.0353, 0.1373, 0.3294, 0.2980, 0.3686, 0.0392, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0314, 0.0353, 0.0353, 0.1412, 0.3294, 0.2980, 0.3686, 0.0392, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0314, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0314, 0.0353, 0.0353, 0.0353, 0.1451, 0.3294, 0.2980, 0.3686, 0.0431,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0314,\n",
       "         0.0314, 0.0353, 0.0353, 0.0353, 0.0353, 0.1529, 0.3255, 0.2980, 0.3647,\n",
       "         0.0510, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0314, 0.0314, 0.0353, 0.0353, 0.0353, 0.0353, 0.1647, 0.3216, 0.2980,\n",
       "         0.3647, 0.0588, 0.0353, 0.0353, 0.0353, 0.0314, 0.0314, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.2000, 0.2314, 0.2627, 0.3098, 0.2627,\n",
       "         0.2588, 0.2824, 0.2980, 0.2549, 0.2196, 0.1216, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0314, 0.0353, 0.0353, 0.0353, 0.0353, 0.0980, 0.0863, 0.0863, 0.0863,\n",
       "         0.0863, 0.0863, 0.0863, 0.0863, 0.0863, 0.0824, 0.0824, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0314, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.2627, 0.3059, 0.2549, 0.2549, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0431, 0.0549, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353, 0.0353,\n",
       "         0.0353]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Flatten()(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\PROJECTs\\Handwritten Digit\\venv\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trans = transforms.Compose([transforms.ToTensor()])\n",
    "img = trans(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gpu = img.cuda()\n",
    "img_gpu.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x15e4b44f140>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = F.to_pil_image(img)\n",
    "plt.imshow(image, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Dự đoán\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m()\n\u001b[0;32m      4\u001b[0m     img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m)\n\u001b[0;32m      5\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(img_gpu)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "# Dự đoán\n",
    "model(in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
